{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The idea of this exercise to perform simple text analysis, a popular concept used in many cutting-edge applications. Also, known as Text Mining - the idea is to retrieve high-quality information from the text. Some of the text mining tasks are: text categorization, text clustering, concept/entity extraction, sentiment analysis, document summarization etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on a custom query, we will try to find the similar documents from our pool of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the text file in zipped format, yes that's possible!\n",
    "t = sc.textFile('test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look how the data looks like\n",
    "t.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopwords: The list of most frequenty used words in a specific language. Stopwords do not offer any useful information about a chunk of text, so we generally remove them from the text before progressing further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to download the list of English stopwords\n",
    "import urllib.request as urllib\n",
    "urllib.urlretrieve (\"https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt\", \"stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = sc.textFile(\"stopwords.txt\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the total dataset into two parts, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = t.randomSplit(weights=[0.9, 0.1], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of partitions\n",
    "train.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the number of partitions\n",
    "train = train.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.getNumPartitions() # Check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.persist() # Store the RDD in memory for quicker operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the text into 'tokens' (individual words) by whitespace\n",
    "traw = train.map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discard the first token(word) and take rest\n",
    "tdata = traw.map(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a function that would make the tokens(words) lowercase and then check if it's a stopword or not.\n",
    "# If stopword, then discard it\n",
    "# Input: x -> list of words/tokens\n",
    "# Outout: list of words/tokens without stopwords\n",
    "def remove_sw(x):\n",
    "    # Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_semi_clean = tdata.map(remove_sw)\n",
    "t_semi_clean.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a function which tries to eliminate all the special characters in tokens(words)\n",
    "# Also, only take words which have length more than 2!\n",
    "# Hint: Use regex, the module in python is re\n",
    "# Input: x -> list of words/tokens\n",
    "# Outout: list of words/tokens with length more than 2 and without any special characters\n",
    "import re\n",
    "def replace_special_chars(x):\n",
    "    # Write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_clean = t_semi_clean.map(replace_special_chars)\n",
    "t_clean.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency (TF): The number of times a specific word occurs in a record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF of term 't' in a document 'd' = Number of times term 't' occurs in a document or record 'd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function which takes the rdd item (record) and \n",
    "# then tries to count the occurances of a specific word in the whole record\n",
    "# Input: record -> list of words/tokens\n",
    "# Output: list of (word, frequency of occurance)\n",
    "def tf(record):\n",
    "    counts = {}\n",
    "    # Write your code here\n",
    "    return list(counts.items()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_with_tfs = t_clean.map(tf)\n",
    "tokens_with_tfs.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency (IDF): How important is a specific word in the whole corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of IDF is not as straightforward as TF. \n",
    "#### IDF score of term 't' = log(total number of documents / number of documents containing 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Take out the unique words per record from 't_clean' \n",
    "# Hint: Use python 'set' function\n",
    "\n",
    "unique_words_per_record = t_clean.map(lambda x: #YOUR CODE HERE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a helper function to attach '1' to every word\n",
    "# Input: record -> list of words\n",
    "# Output: list of tuples where each tuple is (word, 1)\n",
    "def attach_1_to_words(record):\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to attach '1' to each and every word across all records of RDD 'unique_words_per_record'\n",
    "# And Return as a single list. \n",
    "# Which transformation should we use?\n",
    "unique_words_per_record_with_1 = # unique_words_per_record. YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add up the '1's together for same words \n",
    "# which is basically counting the number of documents where a specific word occurs!\n",
    "# Which transformation?\n",
    "tokens_with_docs_count = # unique_words_per_record_with_1. YOUR CODE HERE\n",
    "tokens_with_docs_count.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, count the total number of documents\n",
    "docs = t_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You have the counts for the words in the whole document set, now try to calculate IDF\n",
    "# Hint: use python module \"math\" and then math.log for logarithm\n",
    "# Return: RDD of (token, idf_score)\n",
    "import math\n",
    "tokens_with_idfs = tokens_with_docs_count.map(lambda x: (x[0], math.log(docs/x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the result on the basis of idf scores and take just 10. Which 'action' do we use?\n",
    "tokens_with_idfs.takeOrdered(10, lambda s: s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the idfs for each of the tokens (words) as a python dict (because we need to use it over and over again)\n",
    "tokens_with_idfs_dict = tokens_with_idfs.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF score of a term in a specific document = TF of the term in a specific doc x IDF of the term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the function tfidf which would take the rdd which has the token counts per document\n",
    "# and then muliply with the IDF score of that term\n",
    "# Input: record -> list of (word, term frequency)\n",
    "# Output: list of (word, tfidf score)\n",
    "def tfidf(record):\n",
    "    res = []\n",
    "    #Your code here\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_docs = tokens_with_tfs.map(tfidf)\n",
    "tfidf_docs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate cosine similarity :  measure of similarity of two documents i.e. the document vectors and the query vector. The document vectors are the vector representation of our documents which we have already calculated and the query vector will be calcultated based on a custom query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://en.wikipedia.org/wiki/Cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The cosine similarity function\n",
    "# Input: doc_record: data rdd record, query: query rdd record\n",
    "# Output: tuple of (doc_record, cosine similarity score)\n",
    "def cosine_similarity(doc_record, query):\n",
    "    dot_prod = 0.0\n",
    "    norm_record = []\n",
    "    norm_query = []\n",
    "    for query_term in query:\n",
    "        norm_query.append(query[query_term])\n",
    "    for word_tfidf in doc_record:\n",
    "        word = word_tfidf[0]\n",
    "        tfidf = word_tfidf[1]\n",
    "        norm_record.append(tfidf**2)\n",
    "        \n",
    "        if word in query:\n",
    "            dot_prod += query[word] * tfidf\n",
    "        res = dot_prod / math.sqrt(sum(norm_record)) / math.sqrt(sum(norm_query))\n",
    "        return (doc_record, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tuples_to_dict(record):\n",
    "    output = {}\n",
    "    for word_tfidf in record:\n",
    "        word = word_tfidf[0]\n",
    "        tfidf = word_tfidf[1]\n",
    "        output[word] = tfidf\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def querybuilder(querystr=\"\"):\n",
    "    query_rdd_raw = sc.parallelize([tuple(querystr.split(' '))])\n",
    "    query_sw = query_rdd_raw.map(remove_sw)\n",
    "    query_rs = query_sw.map(replace_special_chars)\n",
    "    query_rdd_tf = query_rs.map(tf)\n",
    "    query_rdd_tfidf = query_rdd_tf.map(tfidf)\n",
    "    query_dict = query_rdd_tfidf.map(tuples_to_dict).collect()[0]\n",
    "    return query_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will build the 'query' which would be used to find similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = querybuilder(\"\") # You can build the query by passing a string OR\n",
    "query = querybuilder(test.take(1)[0])  # Build the query from the test RDD using any of the documents\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = tfidf_docs.map(lambda x: cosine_similarity(x, query)) # Calculate the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.takeOrdered(10, key=lambda s: -s[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The rest of the section is optional and could be used if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.filter(lambda x: x is None).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = r.filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the document id and then sort\n",
    "r.zipWithIndex().takeOrdered(5, key=lambda s: -s[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_original_record_ids(result_rdd, number):\n",
    "    ids = []\n",
    "    r_rdd = result_rdd.zipWithIndex()\n",
    "    r_rdd_sorted = r_rdd.takeOrdered(number, key=lambda s: -s[0][1])\n",
    "    i = 0\n",
    "    for rec in r_rdd_sorted:\n",
    "        ids.append((rec[1], i))\n",
    "        i = i+1\n",
    "    return ids\n",
    "\n",
    "def filter_records_on_ids(training_record, oids):\n",
    "    position = training_record[1]\n",
    "    for oid in oids:\n",
    "        if position == oid[0]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def map_final_records(training_record, oids):\n",
    "    position = training_record[1]\n",
    "    for oid in oids:\n",
    "        if position == oid[0]:\n",
    "            return (training_record, oid[1])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oids = get_original_record_ids(r, 10)\n",
    "oids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the full content of the matched documents\n",
    "train.zipWithIndex().filter(lambda x: filter_records_on_ids(x, oids)).map(lambda x: map_final_records(x, oids)).takeOrdered(10, lambda s: s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
