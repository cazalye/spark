{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy (Numerical Python) - Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_list = [[1,2], [4,9], [10,100]]  # list of list\n",
    "# list_of_list + 1\n",
    "# list_of_list * 2\n",
    "\n",
    "# A numpy array is similar to MATLAB matrix\n",
    "numpy_array = array(list_of_list)\n",
    "print(numpy_array)\n",
    "print('Add 1 to each element')\n",
    "print(numpy_array + 1)\n",
    "print('-------')\n",
    "print('Multiply each element by 2')\n",
    "print(numpy_array * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of array is ', numpy_array.shape)\n",
    "print('Rank of array (matrix) is ', numpy_array.ndim)\n",
    "print('Item size is ', numpy_array.dtype)\n",
    "print('Reshaped Array from 3x2 into 2x3:')\n",
    "print(numpy_array.reshape(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy array operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = array( [[1,1],\n",
    "               [0,1]] )\n",
    "B = array( [[2,0],\n",
    "                   [3,4]] )\n",
    "\n",
    "print('A')\n",
    "print(A)\n",
    "print('B')\n",
    "print(B)\n",
    "print('\\n')\n",
    "print('A*B')\n",
    "print(A*B)\n",
    "print('\\n')\n",
    "print('A-B')\n",
    "print(A-B)\n",
    "print('\\n')\n",
    "print('A.B')\n",
    "print(A.dot(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more statistical functions\n",
    "print('Sum of matrix A is ', A.sum())\n",
    "print('Sum of matrix B is ', B.sum())\n",
    "print('Min element in marix B is ', B.min())\n",
    "print('Max element in marix B is ', B.max())\n",
    "print('Exponential for B is ', numpy.exp(B))\n",
    "print('Exponential for B is ', numpy.sqrt(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/logistic-regression-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark MLLIB Docs: https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the training and test data set in raw form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile('data/kddcup10.data.gz')\n",
    "test_raw_data = sc.textFile('data/kddtest.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.take(10)  # Check how the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.filter(lambda x: 'normal' not in x).take(10)  # Check how the 'not' normal data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the raw training and test data for usage with the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Write down the code for parsing here, \"ONLY\" the numerical columns have to be selected for training purposes!\n",
    "def parse_interaction(line):\n",
    "    \"\"\" Selects the numerical columns and converts the last column (attack type) to either 0.0 or 1.0 (output label)\n",
    "    If the attack type is normal. then label should be 0.0 for every other case it should be 1.0\n",
    "    \n",
    "    Args: line of the data set in a string format\n",
    "    \n",
    "    Returns: LabeledPoint object where the first argument is the output label (Double) and \n",
    "    second argument is an numpy array of remaining feature columns (Double)\n",
    "    \"\"\"\n",
    "     # leave_out columns 1,2,3,41\n",
    "    line_split = line.split(\",\")\n",
    "   \n",
    "    clean_line_split = line_split[0:1]+line_split[4:41]\n",
    "    output_attack = 1.0\n",
    "    if line_split[41]=='normal.':\n",
    "        output_attack = 0.0\n",
    "    \n",
    "    features_numpy_array = array([float(x) for x in clean_line_split])   \n",
    "    return LabeledPoint(output_attack, features_numpy_array)\n",
    "\n",
    "training_data = raw_data.map(parse_interaction)\n",
    "test_data = test_raw_data.map(parse_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Logistic Regression classfier with the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "logit_model = LogisticRegressionWithLBFGS.train(training_data)\n",
    "logit_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the output labels for test datasets with the logistic regression classifier model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_model.predict will predict the output label based on the features given to it\n",
    "original_labels_vs_predicted_labels = test_data.map(lambda p: (p.label, logit_model.predict(p.features)))\n",
    "original_labels_vs_predicted_labels.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the accuracy by selecting only the records where the given labels and predicted labels are equal , and dividing the count of those records by total records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accuracy = original_labels_vs_predicted_labels.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(test_accuracy,4))  # Print the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means Clustering with Mlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/K-means_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark MLLIB Docs: https://spark.apache.org/docs/latest/mllib-clustering.html#k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('data/unbalance.txt')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rdd_split = rdd.map(lambda line: line.split(' '))\n",
    "rdd_split.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = rdd_split.map(lambda x: (float(x[0]), float(x[1]))).cache()  # Select only the features , convert to float\n",
    "output = rdd_split.map(lambda x: int(x[2])).cache()  # Take the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_python_list = features.collect()\n",
    "# Convert it to a numpy array\n",
    "features_numpy_array = array(features_python_list)\n",
    "# Plot the figure using matplotlib library\n",
    "plt.plot(features_numpy_array[:,0], features_numpy_array[:,1], 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the model (cluster the data)\n",
    "cluster_model = KMeans.train(features, 8)  # intialization step is crucial in algorithms which are randomized\n",
    "#cluster_model = KMeans.train(features, 8, initializationSteps=100, epsilon=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = array(cluster_model.clusterCenters)\n",
    "\n",
    "plt.scatter(features_numpy_array[:,0], features_numpy_array[:,1]) # Plot the points first\n",
    "\n",
    "for index in range(0,8):  # For every cluster plot the centers\n",
    "    plt.scatter(cluster_centers[index,0], cluster_centers[index,1], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the output labels using the model generated\n",
    "predicted_labels = array(cluster_model.predict(features).collect())\n",
    "print(predicted_labels)  # have a look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out the unique output labels from array (Using numpy unique function)\n",
    "numpy.unique(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the scatter plot of the points again using the command above but with colors (Hint: use the parameter c to pass color array)\n",
    "plt.scatter(features_numpy_array[:,0], features_numpy_array[:,1], c=predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the computational cost of the cluster\n",
    "cluster_model.computeCost(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
